
\section{Testing}

\subsection{Search Domains Analysis}

After performing the unit testing, the library still has to be proven reliable
and fast in different search domains. To do so, we will use these domains:

\begin{itemize}
\item \textbf{N-Queens}: This problem consists in placing $n$ queens in a $n
  \times n$ chess board so that no two queens are in the same row, column or
  diagonal: that is, placing all the queens without two queens threatening each
  other. This problem is provided in the library under the module
  \texttt{Search.[...].ToyProblem.NQueens}, and this is the implementation that will
  be used to test the performance.
\item \textbf{8-Puzzle}: This problem uses a $3 \times 3$ board that contains 8
  numerated tiles and a blank space. The tiles can be slide around the board if
  they are next to the blank space, and the purpose of the puzzle is to end up
  with a sorted board. This problem is provided in the library under the module
  \texttt{Search.[...].ToyProblem.EightPuzzle}.
\item \textbf{Moving AI Maps}: This problem depends receives a map and two
  coordinates, and the agent generated tries to find a path that goes from one
  point to the other. Depending on the algorithm, this path will be the easier
  to find or the shortest. The map file is expected to be in the format
  provided by \cite{movingai-benchmarks}. This is the most demanding problem of
  all to be tested, as well as the most versatile. A parser for this kind of
  maps is included in \texttt{Search.[...].ToyProblem.MapParser}.
\end{itemize}

In each problem, a different capability of the library is tested, and it will
be explained in depth in each of the domains subsections. To perform these
tests, the included tools with the library will be used to perform all
measurements.\\

\subsubsection{Results \& Performance in N-Queens}

The implementation for the N-Queens problem included with the library uses as
operators the fact of adding a queen to a new column in all possible positions.
That way, the approach followed to solve this problem is a brute-force
approach, which is ideal to study all the uninformed algorithms. On the other
hand, there is no heuristic available for this approach, so the informed set of
algorithms cannot be tested in this domain. For the same reason, no correctness
of the solution has to be considered: if the solution is found is because it is
valid. That makes the evaluation simpler to understand for this case.\\

\begin{figure}[!htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      axis lines*=left,
      every axis plot post/.style={/pgf/number format/fixed},
      enlargelimits=0.15,
      x = 2.3cm,
      bar width = 0.8cm,
      legend style={at={(0.5,-0.15)},
        anchor=north,legend columns=-1},
      ylabel={Time (ms)},
      symbolic x coords={BFS,DFS,IDFS(1),IDFS(5),UCS},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      ]
      \pgfplotsset{cycle list name=list}
      \addplot coordinates {
        (BFS,125.5)
        (DFS,6.806)
        (IDFS(1),352.0)
        (IDFS(5),38.0)
        (UCS,137.1)
      };
      % \legend{}
    \end{axis}
  \end{tikzpicture}
  \caption{Time to find a solution in 8-Queens}
  \label{nq:time}
\end{figure}


\begin{figure}[!htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      compat=newest,
      axis lines*=left,
      every axis plot post/.style={/pgf/number format/fixed},
      ybar,
      enlargelimits=0.15,
      x = 2.3cm,
      bar width = 0.8cm,
      legend style={at={(0.5,-0.15)},
        anchor=north,legend columns=-1},
      ylabel={Nodes expanded},
      symbolic x coords={BFS,DFS,IDFS(1),IDFS(5),UCS},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      ]
      \pgfplotsset{cycle list name=list}
      \addplot coordinates {
        (BFS,1966)
        (DFS,114)
        (IDFS(1),5623)
        (IDFS(5),650)
        (UCS,1966)
      };
      % \legend{}
    \end{axis}
  \end{tikzpicture}
  \caption{Nodes expanded to find a solution in 8-Queens}
  \label{nq:nodes}
\end{figure}


\begin{figure}[!htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      compat=newest,
      ybar,
      axis lines*=left,
      every axis plot post/.style={/pgf/number format/fixed},
      enlargelimits=0.15,
      x = 2.3cm,
      bar width = 0.8cm,
      legend style={at={(0.5,-0.15)},
        anchor=north,legend columns=-1},
      ylabel={Total Memory Allocated (KB)},
      symbolic x coords={BFS,DFS,IDFS(1),IDFS(5),UCS},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      ]
      \pgfplotsset{cycle list name=list}
      \addplot coordinates {
        (BFS,72840)
        (DFS,6066)
        (IDFS(1),152132)
        (IDFS(5),20164)
        (UCS,68501)
      };
      % \legend{}
    \end{axis}
  \end{tikzpicture}
  \caption{Total memory allocated to find a solution in 8-Queens}
  \label{nq:memory}
\end{figure}


We can see a clear correlation between the time taken find a solution (Figure
\ref{nq:time}) and the nodes expanded in the process (Figure \ref{nq:nodes}).
The results are not surprising: We can see how Breadth-First Search takes some
a fair amount of time to find the solution, but the algorithm has to traverse
completely some depths of the tree that cannot contain solutions, which makes
a handicap for this algorithm. It is also noticeable that the best algorithm
for this brute-force search is Depth-First Search, which is the fastest
algorithm. Also an important insight is the fact that Iterative-Deepening
Depth-First Search is quite sensitive to the parameters given: appropriate step
parameters can save up to an order of magnitude in the nodes expanded.
Uniform-Cost Search also behaves as a Breadth-First Search, since there is no
variable cost function for this concrete example. In the Figure
\ref{nq:memory}, we can see the total amount of memory allocated during the
process; not the total memory used by it (memory is claimed back at some
moments when it stops being used). The correlation between nodes and memory
allocated is clear as well.\\

It is also interesting to check the results of the benchmarks for counting all
the solutions in the search domain. This test is specially interesting with
Breadth-First Search and Depth-First Search: both of them should find the same
number of solutions (92 for 8-Queens) and expand all nodes in the search space.
The differences between timing will show if there is an intrinsic difference in
the language in both approaches:\\

\begin{lstlisting}
<lambda> benchmark $ withAllNQueens [("bfs", bfs), ("dfs", dfs)]
benchmarking nQueens/bfs
time                 116.3 ms   (113.9 ms .. 119.5 ms)
                     0.999 R²   (0.998 R² .. 1.000 R²)
mean                 116.6 ms   (115.9 ms .. 117.6 ms)
std dev              1.247 ms   (772.9 µs .. 1.917 ms)
variance introduced by outliers: 11% (moderately inflated)

benchmarking nQueens/dfs
time                 111.9 ms   (110.4 ms .. 114.1 ms)
                     1.000 R²   (0.999 R² .. 1.000 R²)
mean                 113.3 ms   (112.6 ms .. 114.1 ms)
std dev              1.149 ms   (803.0 µs .. 1.613 ms)
variance introduced by outliers: 11% (moderately inflated)
\end{lstlisting}

The results show that both algorithms find the set of solutions using almost
the same amount of time: Depth-First Search is just 2.5\% faster than
Breadth-First, probably because of the time saved in allocating memory for the
data structure, which is substituted by stack calls. These are good news: In
this search domain, when finding all possible solutions, using linear search
memory or a general search with a data structure have no visible time overhead
from one another.\\

\subsubsection{Results \& Performance in 8-Puzzle}

In the implementation available in the library for 8-Puzzle, the operators
consists of moving the blank space in the board. It is just a simpler way to
express, the problem, and it works the same way as moving the tiles to the
blank space. Thus, the complexity in this puzzle is much higher than in
N-Queens, where a brute force search was proven to be quite efficient.
Fortunately, there is an advantage compared to the previous problem: we can
design heuristic functions to help us with it, and use the informed set of
algorithms at hand. Included in the module there is a basic heuristic for this
problem: the hamming heuristic, which counts how many numbers are not in the
correct position.\\

It is important to notice that the initial position of the board to perform
these tests is the one provided in the library for benchmarking puzzles. This
initial position has an optimal solution of 17 steps before being solved. We
will then specify the correctness of the solutions found by the algorithm, but
first we can check the times taken by each of them.\\

\begin{figure}[!htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      compat=newest,
      every axis plot post/.style={/pgf/number format/fixed},
      ybar=5pt,
      enlargelimits=0.28,
      x = 1.5cm,
      ymax = 1600,
      bar width = 0.8cm,
      legend style={at={(0.5,-0.15)},
        anchor=north,legend columns=-1},
      ylabel={Time (ms)},
      symbolic x coords={BFS,DFS,IDFS,HC,A*,IDA*,BNB},
      xtick=data,
      restrict y to domain*=0:2300,
      visualization depends on=rawy\as\rawy,
      after end axis/.code={ % Draw line indicating break
        \draw [ultra thick, white, decoration={snake, amplitude=1pt},
        decorate] (rel axis cs:0,1.05) -- (rel axis cs:1,1.05);
        },
    nodes near coords={%
            \pgfmathprintnumber{\rawy}% Print unclipped values
          },
      axis lines*=left,
      clip=false
      ]
      \pgfplotsset{cycle list name=list}
      \addplot coordinates {
        (BFS,1327)
        (DFS,59.53)
        (IDFS,1464)
        (HC,701.2)
        (A*,234.6)
        (IDA*,113.4)
        (BNB,44396)
      };
      % \legend{}
    \end{axis}
  \end{tikzpicture}
  \caption{Time to find a solution in 8-Puzzle}
  \label{8p:time}
\end{figure}

In the Figure \ref{8p:time} we can see the times taken to solve this puzzle.
The uninformed algorithms perform as expected: Breadth-First Search takes a
fair amount of time but ensures that the solution is optimal, while Depth-First
Search takes a short time to find the first solution available. Iterative
Deepening Depth-First Search performs worse than BFS without any warranties,
but it can be tuned later with better parameters to improve the times
obtained.\\

On the other hand, we see for the first time results on the informed
algorithms. Hill Climbing takes almost half the time taken by BFS, but it is
important to notice that it is simply following greedily the heuristic
function, so it will not always return the optimal solution. On the other hand,
A* uses both the cost function and the heuristic function, ensuring that the
obtained solution is the optimal. Also, A* performs really good: it's around
seven times faster than BFS, and the results are indeed the same.
Iterative-Deepening A*, on the other hand, uses linear memory, but if the
parameters are not correct the solution found it's not going to be optimal. We
can see that IDA* is twice as fast as A*.\\

However, we can see that Depth-First Branch \& Bound is much slower than the
rest of the available algorithms. Although it produces the optimal solution, it
is several orders of magnitude slower, which makes it not acceptable. The
reason for this is that the rest of the algorithms are obtaining one single
solution: The first solution that they are able to find. On the other hand,
Branch \& Bound uses the first solution as a bound, and then keeps on searching
until the solution space is completely exhausted. That makes BNB not being able
to take advantage of Haskell's laziness as the other algorithms, and resulting
in a terrible performance in this search domain, since the search space is hard
to completely explore. That makes Depth-First Branch \& Bound a bad choice for
this problem.\\

\begin{figure}[!htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      compat=newest,
      every axis plot post/.style={/pgf/number format/fixed},
      ybar=5pt,
      enlargelimits=0.21,
      x = 1.5cm,
      bar width = 0.7cm,
      axis lines*=left,
      legend style={at={(0.5,-0.15)},
        anchor=north,legend columns=-1},
      ylabel={Steps},
      symbolic x coords={BFS,DFS,IDFS,HC,A*,IDA*,BNB},
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      ]
      \pgfplotsset{cycle list name=list}
      \addplot coordinates {
        (BFS,17)
        (DFS,1053)
        (IDFS,18)
        (HC,88)
        (A*,17)
        (IDA*,17)
        (BNB,17)
      };
      % \legend{}
    \end{axis}
  \end{tikzpicture}
  \caption{Steps to solve the problem in each solution found}
  \label{8p:steps}
\end{figure}

In Figure \ref{8p:steps} we can see the quality of each algorithm's answer to
the problem. The shorter the solution found, the better the solution is, and
the optimal solution for the instance being tested is 17 steps. We can see how,
in this example, the algorithms that are able to find that solution are BFS,
A*, IDA*, and BNB; with IDFS is really close to it. Unsurprisingly, the greedy
algorithms (DFS and Hill Climbing) do not find optimal solutions, but their
time/memory performance is overall better.\\

\subsubsection{Results \& Performance in Moving AI Maps}

The Moving AI maps are the most complex and demanding test of all. These maps
are formed by a $512 \times 512$ grid that contain walls and different sets of
terrains. The first test, to test the timing and different behaviors of the
algorithms, will be performed in a maze with corridors of width 1. That means
that the problems only have to check paths at different intersections, making
the branching factor smaller. There is a single solution, with a length of 5172
nodes until the goal state. The results to this test are:\\

\begin{figure}[!htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      compat=newest,
      every axis plot post/.style={/pgf/number format/fixed},
      ybar=5pt,
      enlargelimits=0.21,
      x = 1.5cm,
      bar width = 0.7cm,
      legend style={at={(0.5,-0.15)},
        anchor=north,legend columns=-1},
      ylabel={Time (ms)},
      symbolic x coords={BFS,DFS,IDFS,HC,A*,IDA*,BNB},
      xtick=data,
      axis lines*=left,
      nodes near coords,
      nodes near coords align={vertical},
      ]
      \pgfplotsset{cycle list name=list}
      \addplot coordinates {
        (BFS,0.559)
        (DFS,0.256)
        (IDFS,10.126)
        (HC,7.453)
        (A*,20.014)
        (IDA*,0.684)
        (BNB,0.392)
      };
      % \legend{}
    \end{axis}
  \end{tikzpicture}
  \caption{Time to find a solution in the $512 \times 512$ maze}
  \label{mp:time}
\end{figure}

There are three things that catch our eye in these results: the great
discordance between the two iterative-deepening methods (applied with similar
parameters to the problem), the great performance of Depth-First Branch \&
Bound and the terrible performance of A*.\\

The first one can be explained in function of the maze itself. It is (in this
case) much more effective to follow greedily the heuristic function rather than
expanding the search tree depth-first. This is translated in IDA* needing much
less iterations than IDFS. However, this contradicts the fact that BFS is much
more performant than Hill Climbing. We will come back to this detail.\\

Second, Depth-First Branch \& Bound has proven to be a good option to solve
this concrete problem, being only slower than DFS. This also ensures that the
problem the algorithm suffered in the previous section are not intrinsic to the
implementation, but to the nature of the problem and the algorithm.\\

But the really surprising measure is A*: how can the algorithm that was the
better performing until now, suddenly performs much more slower than the rest
of algorithms. How is it possible than an A* search is an order of magnitude
slower than its BFS counterpart? After all the previous testing on the code, it
is possible to state that concrete conditions are resulting in slow conditions
for A*. Incidentally, there is also a noticeable slower performance for Hill
Climbing, specially after noticing that it should be favored by the heuristic
function in this scenario. This two algorithms share something in common: both
of them are implemented using a priority queue
(\texttt{Search.[...].DataStructure.PriorityQueue}). Knowing this, our next
step is to try to figure out what is causing this degradation for the priority
queue or if it is something else. This will be studied more comprehensively in
the following profiling section.\\

Furthermore, if the Moving AI Benchmarks can find the real Achilles' heel of
the library: due to its purpose of finding the best $k$ solutions for a given
problem, when we try to run demanding pathfinding problems in corridors of
width $>1$, the algorithms revisits the same nodes as long as its own path does
not contain it. In problems based in implicit graphs, it usually means an
admissible redundancy; in problems based in maps this implies that the number
of paths being tracked is higher and in algorithms like Breadth-First Search
(that expands the nodes with lower depth first) the algorithm struggles to
advance to the final solution \cite{dechter-2012-search}. For that reason, it
is recommended to use a traditional approach for different maps and mazes.\\

\subsection{Unit \& Integration Testing}

Thanks to the Cabal packaging tool, it is easy to include a test suite in the
library, that can be run automatically. The chosen way to test the functions of
the library was to create a \texttt{Test} folder, in which replicate the
structure of the library modules. In each of this new modules there should be
all the tests for the functions included in them. Since the design of the
library functions are incremental, these small specifications can be both unit
tests (for instance, to check if \texttt{Search.DataStructure.Queue} correctly
adds a node) or integration tests (if a queue and a general search can be
combined correctly to work as a Breadth-First Search). To perform the latter
tests, we will use small examples of each of the search domains that have been
included in the library as toy problems, to test the correctness of the results
returned.\\

To perform this task, the easiest way is to use the \texttt{hspec} package,
that offers all the necessary functions to perform the testing of the modules,
plus a function to discover and run all necessary \texttt{Spec} files when
asking the library to be tested. That way, we can just create new test modules
and they will be automatically discovered. To run the tests it is only
necessary to build the library along with the tests (\texttt{cabal build
  --enable-tests}) and then run \texttt{cabal test}. This convenient access to
the tests is the cornerstone for the Continuous Integration of the library,
that will be specified later on the document.\\

Running this command in the version for this thesis (\texttt{0.2.0.0}) runs a
total of 156 different examples (both unit and integration tests) in
$\sim 2.6581$ seconds with 0 failures. The output presents the name of each of
the modules being tested, along with a description of each of the tests and a
mark in case there was a failure in such example. The complete set of tests can
be found in the path \texttt{src/Test/} of the library code.\\

\subsection{Profiling}

As seen in previous sections, the library has an overall good performance, but
under certain conditions the it becomes drastically slower. However, it is hard
to say what is exactly happening: due to the side-effects isolation, there is
no easy way to use print-debugging or to time different functions of the code
to see what is exactly going on. At this level of abstraction, what the
computer is doing in the bad performances is, indeed, completely hidden from
us.\\

However, the language provides certain tools to profile code if it is indeed
necessary to spot a problem. The main tool for profiling is using the
\texttt{+RTS} interface, that will be detailed next. After explaining the
process of creating a working environment to profile the project, we will
undergo several experiments to find the hot spots in the project and follow a
procedure to fix it.\\

\subsubsection{Creating the \texttt{profiling} executable}

To effectively profile code in Haskell, it is needed to make use of an
interface to the Runtime System (RTS). This system, which is not usually
intended to be directly queued, is the one in charge of a lot of Haskell's nuts
and bolts: RTS includes a storage manager, a scheduler and a profiler, among
other necessary runtime services that work as a middle system between the user
code and the compiled Haskell code \cite{ghc}.\\

To effectively profile the code, it is necessary to use both compilation and
runtime flags. This is usually easy to manage with single files, but a project
like this can become really hard to compile with a simple \texttt{ghc} command
outside of a packaging tool like Cabal or Stack. Plus, the library needs to
install the profile version of the dependencies, so simply compiling by hand
may not be enough to successfully profile the code.\\

The best solution available for a project this size is to create a Cabal
executable that will gather all the options, and enables for an easy access to
the profiling of the library. To create this executable, we need to add the
Listing \ref{prof-cabal} to the file \texttt{agis.cabal} in the root of the
project.\\

\begin{lstlisting}[style=haskell, label=prof-cabal, caption=
Setup for the \texttt{profiling} executable in the Cabal file]
-- Library profiling executable
executable profiling

  hs-source-dirs:      src/Search/Pure
           
  main-is:             Profiling.hs
  
  ghc-options:         -O2
                       -threaded
                       "-with-rtsopts=-N -p -s -h -i0.1"

  build-depends:       [...]

  default-language:    Haskell2010
\end{lstlisting}

This configuration creates a new executable called \texttt{profiling}, that
will call the file \texttt{Profiling.hs} in the directory
\texttt{search/Search/Pure}. This file will simply contain a main method
calling the code to be tested, which will be the main method for the Moving AI
maps. For this executable to created, we pass several options to the compiler:
to be compiled with level 2 optimizations (\texttt{-O2}) and using the
concurrent abstractions (\texttt{-threaded}), as well as to include all the
necessary runtime options (\texttt{-with-rtsopts}):

\begin{itemize}
\item \texttt{-N} sets the number of available processors to the number of
  processors of the system.
\item \texttt{-p} generates the profiling report.
\item \texttt{-s} includes garbage collection in the report.
\item \texttt{-h} includes memory usage in the report.
\item \texttt{-i} sets the profiler frequency to 0.1 seconds.  
\end{itemize}

Running \texttt{cabal build} will compile the project (and install all the
necessary profiling version of the dependencies) and generate the executable.
To run it, it is only necessary to run \texttt{cabal run profiling}. This
shortcut is much more robust and simple than to input the compiler flags every
time. Once run, it outputs a brief description of the profiling in the memory
and generates a comprehensive report (a \texttt{.prof} file) where the runtime
statistics are detailed. Now that the environment is set, we can proceed to
profile the code searching for the hot spots in the library.\\


\subsubsection{Finding the hot spots of the library}

As it could be seen when testing the library with the Moving AI maps, there is
a problem related to the priority queue implementation of the library. There
are several possible causes for this, and to better find out what the problem
may be, we need to use the aforementioned profiler. An open map will be used to
profile the code, and try to see where the hot spots of time and memory
allocation are.\\

To do so, it is necessary to use GHC's cost centres: A special annotation that
precedes a statement and lets the compiler know that it is a point of interest.
All the cost centres will then be collected in the \texttt{.prof} report, that
contains a comprehensive analysis of how the code is behaving \cite{ghc}. A
cost centre is included in the code using \texttt{\{-\# SCC "name" \#-\}
  <expression>}, and it will track and include in the report the total time and
memory usage of the expression under the row \texttt{name}.\\

Running the example with some functions being tracked results in the following
report:\\

\begin{lstlisting}
	Sun Jul 23 15:28 2017 Time and Allocation Profiling Report  (Final)

	   profiling +RTS -N -p -s -h -i0.1 -RTS

	total time  =        3.48 secs   (3479 ticks @ 1000 us, 1 processor)
	total alloc = 2,730,181,144 bytes  (excludes profiling overheads)

COST CENTRE   MODULE                           %time %alloc

union         Data.Heap.Internal                47.1   30.3
getCell       Search.Pure.ToyProblem.MapParser   8.8    5.0
move          Search.Pure.ToyProblem.MapParser   6.7    9.5
visited       Search.Pure.General                6.6    0.3
generalSearch Search.Pure.General                5.7   13.0
fromList      Data.Heap                          4.7   10.8
expand        Search.Pure.General                4.7    6.4
size          Data.Heap.Internal                 3.3    7.7
rank          Data.Heap.Internal                 2.6    7.4
expand        Search.Pure.General                1.3    4.1
cost          Search.Pure.General                1.1    0.9
heuristic     Search.Pure.General                1.1    0.4
hash          Data.HashMap.Base                  1.1    0.0
readWMap      Search.Pure.ToyProblem.MapParser   0.3    1.6
\end{lstlisting}

We can see how more than half of the time is being consumed in the library
calls of \texttt{Data.Heap}. This is provided by the package \texttt{heap},
which allows import a \texttt{MinHeap}, the basic data structure on top of
which is built our own implementation of a purely functional priority queue,
as described in \cite{okasaki-1999-purely}. We can see that the library is
generating a time overhead that is not acceptable, but apart from the report,
there is a key statement in the RTS briefing that appears in the prompt:\\

\begin{lstlisting}
> cabal run profiling
   4,668,660,936 bytes allocated in the heap
   4,684,133,240 bytes copied during GC
     183,250,168 bytes maximum residency (44 sample(s))
       1,394,480 bytes maximum slop
             371 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      8972 colls,     0 par    0.998s   1.024s     0.0001s    0.0031s
  Gen  1        44 colls,     0 par    5.425s   5.637s     0.1281s    0.2877s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.001s  (  0.013s elapsed)
  MUT     time    3.390s  (  4.697s elapsed)
  GC      time    5.127s  (  5.354s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    1.296s  (  1.306s elapsed)
  EXIT    time    0.003s  (  0.051s elapsed)
  Total   time    9.818s  ( 10.116s elapsed)

  Alloc rate    1,377,217,805 bytes per MUT second

  Productivity  34.6% of total user, 34.0% of total elapsed
\end{lstlisting}

In there, we can see how the \texttt{MUT} (actual computations that are being
issued) take 3.390s; but \texttt{GC} (the garbage collector, that claims back
the memory not being used) is generating more than 5 seconds in pauses.
Overall, the productivity of the code (the fraction of the execution that is
invested in the actual computations) is 34.6\%: only one out of each three
seconds of execution. The main responsible for this situation is the garbage
collector, and there are two main possibilities for this: the library code is
being misused and the compiler is not able to execute the proper optimizations
on it, or the garbage collection settings are affecting the performance.\\

\subsubsection{Tuning the garbage collection}

After running the profiling experiments in the code, we have found that
problems with a fast growing queue generate a problematic space overhead, that
is translated to big garbage collection pauses that make execution slow. The
cost centers reveal that the space allocation happen most of all in the
\texttt{DataStructure} methods for adding nodes, so let us keep researching how
to fix this issue. To generate a lot of nodes in the open list, we will use a
bad heuristic in the problem \texttt{MapParser}, to get a solution with these
statistics (obtained with the \texttt{Search.Monadic} library):

\begin{itemize}
 \item Number of expanded nodes: 378790
 \item Number of enqueued nodes: 734887
 \item Maximum length of the queue: 356099
\end{itemize}

Just to obtain a plan length of 19. This big size for the open list will
trigger the garbage collection problem and let us study it better. The mean
execution time for this problem in the original implementation is 11.573
seconds; which may seen acceptable but increases rapidly with the plan length
(a new node can add 8-10 seconds to this execution time).\\

Now, to experience the impact of the garbage collector settings we will use a
tool called \texttt{ghc-gc-tune} \cite{ghc-gc-tune}. This tool will allow us to
brute force the garbage collector options and get a general insight of the
behavior of the program under different settings. This will let us see if the
problem is originated by the garbage collector being too intrusive or if there
is a reason intrinsic to the implementation of the program. The tool needs to
be fed an executable with the RTS options open for modification, we need to
design a new executable:\\

\begin{lstlisting}[style=haskell, label=gc-cabal, caption=
Setup for the \texttt{gc-tuning} executable in the Cabal file]
-- Library garbage collection tuning executable
executable gc-tuning

  hs-source-dirs:      src/Search/Pure
           
  main-is:             Profiling.hs
  
  ghc-options:         -O2
                       -threaded
                       -rtsopts

  build-depends:       [...]

  default-language:    Haskell2010
\end{lstlisting}

This executable uses the file \texttt{Search/Pure/Profiling.hs}, as well as the
previous executable used, but this time the RTS options are left open instead
of providing some. This is necessary because the access to the RTS is
considered a vulnerability and it is disabled for all packages compiled by
default. Once we have linked with \texttt{-rtsopts}, we can run the tool. After
a long execution trace, the following results are returned:\\

\begin{lstlisting}
Best settings for Running time:
5.43s:  +RTS -A16384 -H536870912
5.49s:  +RTS -A536870912 -H1073741824
5.52s:  +RTS -A268435456 -H536870912
5.68s:  +RTS -A4194304 -H536870912
5.68s:  +RTS -A268435456 -H1073741824
\end{lstlisting}

Under some garbage collection settings, it is possible to improve the total
execution time in a 50\%. Without having to refactor the code, it is possible
to cut in half the time just by setting some options in the garbage collection.
Although this results are good, let us not be over-optimistic: by tuning this
executable we may be making the execution options specific to the problem that
is being used, instead of obtaining a general improvement. Over-fitting the
library to this concrete example is at risk.\\

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{img/gc-tuning.png}
\caption{Garbage collection flags compared to execution time.}
\label{gc-tuning}
\end{figure}

To check if that is indeed the case, we can take a look at the general spectrum
of results generated by the tool (Figure \ref{gc-tuning}). In there, we can see
that the garbage collection settings are of great importance for the library:
in the plane that represents the different execution times, a flat shape would
indicate that there is not much influence done by the garbage collection. In
the plot we can see the opposite: a strenuous surface that represents the time
the execution expends on garbage collection, on how a difference in the
allocation space or heap available can increase up to a 40\% the total
computation time. In the results, there is an expected peak when the garbage
collection settings force the use of low memory, and its counterpart when the
settings are high. Although it may be tempting to offer the program the highest
memory flags to be sure of low execution times, this may be counter-productive
in lower-memory systems and parallel applications.\\

To check if the flags are, indeed, being a benefit for a general number of
nodes expanded, the Figure XXXXX shows a side by side plot of different plan
lengths in the same problem, with and without using any garbage collection
flags.

%%% TODO:

\subsubsection{Profiling different \texttt{PriorityQueue} implementations}


Now that we have discarded a substantial improvement by tuning with flags the
garbage collection, it's time to analyze the \texttt{PriorityQueue}
implementation and try to find if there is something to be improved in the
code. The code implements the priority queue over a package called
\texttt{heap}, which promises to deliver ``\textit{a flexible implementation of
  min-, max-, min-priority, max-priority and custom-priority heaps based on the
  leftist-heaps from Chris Okasaki's book Purely Functional Data Structures}''
\cite{okasaki-1999-purely}, which was exactly what was desired when
implementing the library (to follow the explanations in the literature).
However, this package may not be as suitable as thought for our purposes or it
may be a misuse of it. Running the \texttt{profiling} executable of the library
in this problem outputs, we can see these results (please bear in mind the
profiling overhead, stated as \texttt{PROF} time):\\

\begin{lstlisting}
> cabal run profiling
   4,452,597,512 bytes allocated in the heap
   5,673,545,840 bytes copied during GC
     234,847,840 bytes maximum residency (45 sample(s))
       1,616,992 bytes maximum slop
             472 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      8573 colls,     0 par    1.030s   1.050s     0.0001s    0.0004s
  Gen  1        45 colls,     0 par    7.107s   7.359s     0.1635s    0.3832s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.001s  (  0.010s elapsed)
  MUT     time    3.333s  (  5.247s elapsed)
  GC      time    6.249s  (  6.495s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    1.888s  (  1.914s elapsed)
  EXIT    time    0.003s  (  0.046s elapsed)
  Total   time   11.475s  ( 11.797s elapsed)

  Alloc rate    1,335,764,960 bytes per MUT second

  Productivity  29.1% of total user, 28.6% of total elapsed
\end{lstlisting}


We can see how the profiler returns that only a 29.1\% of the time has been
spent in actual search computations, while more than half of the execution time
(54.5\%) has been spent in garbage collection. To check if this space leak is
caused by our implementation or by the library used to implement the priority
queue, we can experiment with different methods.\\

We can run the same problem using the implementation with the \texttt{psqueues}
(and more accurately the \texttt{Data.IntPSQ} module), an alternative library
that offers an implementation of a higher level than our original \texttt{heap}
package. The results are as follow:\\


\begin{lstlisting}
> cabal run profiling
   4,867,074,880 bytes allocated in the heap
   4,823,508,568 bytes copied during GC
     183,851,592 bytes maximum residency (46 sample(s))
       1,405,912 bytes maximum slop
             373 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      9345 colls,     0 par    0.992s   1.017s     0.0001s    0.0018s
  Gen  1        46 colls,     0 par    5.654s   5.849s     0.1271s    0.2936s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.001s  (  0.012s elapsed)
  MUT     time    3.508s  (  4.884s elapsed)
  GC      time    5.292s  (  5.498s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    1.354s  (  1.367s elapsed)
  EXIT    time    0.003s  (  0.037s elapsed)
  Total   time   10.159s  ( 10.431s elapsed)

  Alloc rate    1,387,545,376 bytes per MUT second

  Productivity  34.6% of total user, 34.1% of total elapsed
\end{lstlisting}


There, it can be seen that the garbage collection time has been reduced by a
second, but it is still a marginal improvement. Therefore, maybe a different
approach is needed: If we take a close look to the \texttt{addList} method, the
main cost center of this problem, we can see that we have two main options to
implement it: the first of them is to create a new \texttt{MinPrioHeap} and use
the library function \texttt{union} to merge both of them inside the new
structure. The code is shown in the listing \ref{addlist:union}.\\

\begin{lstlisting}[style=haskell, caption= \texttt{addList} using
  \texttt{union}, label=addlist:union]
addList xs (PriorityQueue h f) = PriorityQueue (union h xs') f
  where xs' = fromList $ map (\n -> (f n, n)) xs
\end{lstlisting}

Although the language should manage the memory correctly, maybe this is not the
best way of managing this operation. As a possible solution, we can try to
force the reutilization of the old structure by adding the nodes using a strict
fold over the \texttt{MinPrioHeap}. This implementation is shown in the listing
\ref{addlist:fold}.\\

\begin{lstlisting}[style=haskell, caption= \texttt{addList} using
  \texttt{fold'}, label=addlist:fold]
addList xs (PriorityQueue h f) = PriorityQueue (foldl' insertNode h xs) f
  where insertNode acc x = insert (f x, x) acc
\end{lstlisting}

To check the performance of all possibilities, the profiling problem stated at
the beginning of this subsection has been run using all combination of
implementations and library, and the full results of performance are found in
the chart shown in the figure \ref{chart-libraries}.\\

\begin{figure}[ht]
  \centering
\begin{tikzpicture}
  \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    axis x line       = none,
    % width = 8cm, height = 4cm,
    ytick = data,
    xmin=0,
    tickwidth         = 0pt,
    enlarge y limits  = 1,
    enlarge x limits  = 0.02,
    symbolic y coords = {Package A,Package B},
    nodes near coords,
    ]
    \pgfplotsset{cycle list name=list}
  \addplot coordinates { (11.779,Package A) (11.046,Package B) };
  \addplot coordinates { (12.473,Package A) (10.677,Package B) };
  \legend{Using union, Using a fold}
  \end{axis}
\end{tikzpicture}
\vspace{-1cm}
\caption{Comparing Package A (\texttt{heap}) and Package B
  (\texttt{psqueues}).}
\label{chart-libraries}
\end{figure}

It can be seen how the performances are very similar to each other: no matter
which library or what implementation we use. We can then conclude that the
problem is intrinsic to the memory management performed by the RTS interface in
Haskell, or by how the compiler is turning the high-level declarations into
machine code.\\

Nonetheless, taking into account the knowledge that has been obtained by
profiling the data structure, the most intuitive and possible reason for this
space leak is that in Haskell, all data in immutable, and by adding the nodes
to the queue a complete new queue is being generated, allocating a new copy of
it and forcing the garbage collector to claim the old queue's space. Although
this is the natural behavior expected from the language, there are workarounds
to make a data structure mutable, but it has been impossible to find an
implementation that allows for a low complexity insertion and extraction of
nodes in a sorted heap and that is mutable. Implementing such data structure in
Haskell is not an easy task, and it is considered to be out of the scope of
this thesis.\\

\newpage

%%% Local Variables:
%%% TeX-master: "tfg"
%%% End: