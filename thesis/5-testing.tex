\section{Testing}

\subsection{Unit testing}

\subsection{Search Domains Analysis}

After performing the unit testing, the library still has to be proven reliable
and fast in different search domains. To do so, we will use these domains:

\begin{itemize}
\item \textbf{N-Queens}: This problem consists in placing $n$ queens in a $n
  \times n$ chess board so that no two queens are in the same row, column or
  diagonal: that is, placing all the queens without two queens threatening each
  other. This problem is provided in the library under the module
  \texttt{Search.ToyProblem.NQueens}, and this is the implementation that will
  be used to test the performance.
\item \textbf{8-Puzzle}: This problem uses a $3 \times 3$ board that contains 8
  numerated tiles and a blank space. The tiles can be slide around the board if
  they are next to the blank space, and the purpose of the puzzle is to end up
  with a sorted board. This problem is provided in the library under the module
  \texttt{Search.ToyProblem.EightPuzzle}.
\item \textbf{Moving AI Maps}: This problem depends receives a map and two
  coordinates, and the agent generated tries to find a path that goes from one
  point to the other. Depending on the algorithm, this path will be the easier
  to find or the shortest. The map file is expected to be in the format
  provided by \cite{movingai-benchmarks}. This is the most demanding problem of
  all to be tested, as well as the most versatile.
\end{itemize}

In each problem, a different capability of the library is tested, and it will
be explained in depth in each of the domains subsections.\\

\subsubsection{Results \& Performance in N-Queens}

\subsubsection{Results \& Performance in 8-Puzzle}

\subsubsection{Results \& Performance in Moving AI Maps}


\subsection{Profiling}

As seen in the previous section, the library has an overall good performance,
but under certain conditions the it becomes drastically slower. However, it is
hard to say what is exactly happening: due to the side-effects isolation, there
is no easy way to use print-debugging or to time different functions of the
code to see what is exactly going on. At this level of abstraction, what the
computer is doing in the bad performances is, indeed, completely hidden from
us.\\

However, the language provides certain tools to profile code if it is indeed
necessary to spot a problem. The main tool for profiling is using the
\texttt{+RTS} interface, that will be detailed next. After explaining the
process of creating a working environment to profile the project, we will
undergo several experiments to find the hot spots in the project and follow a
procedure to fix it.\\

\subsubsection{Creating the \texttt{profiling} executable}

To effectively profile code in Haskell, it is needed to make use of an
interface to the Runtime System (RTS). This system, which is not usually
intended to be directly queued, is the one in charge of a lot of Haskell's nuts
and bolts: RTS includes a storage manager, a scheduler and a profiler, among
other necessary runtime services that work as a middle system between the user
code and the compiled Haskell code \cite{ghc}.\\

To effectively profile the code, it is necessary to use both compilation and
runtime flags. This is usually easy to manage with single files, but a project
like this can become really hard to compile with a simple \texttt{ghc} command
outside of a packaging tool like Cabal or Stack. Plus, the library needs to
install the profile version of the dependencies, so simply compiling by hand
may not be enough to successfully profile the code.\\

The best solution available for a project this size is to create a Cabal
executable that will gather all the options, and enables for an easy access to
the profiling of the library. To create this executable, we need to add the
Listing \ref{prof-cabal} to the file \texttt{agis.cabal} in the root of the
project.\\

\begin{lstlisting}[style=haskell, label=prof-cabal, caption=
Setup for the \texttt{profiling} executable in the Cabal file]
-- Library profiling executable
executable profiling

  hs-source-dirs:      src/Search/Pure
           
  main-is:             Profiling.hs
  
  ghc-options:         -O2
                       -threaded
                       "-with-rtsopts=-N -p -s -h -i0.1"

  build-depends:       [...]

  default-language:    Haskell2010
\end{lstlisting}

This configuration creates a new executable called \texttt{profiling}, that
will call the file \texttt{Profiling.hs} in the directory
\texttt{search/Search/Pure}. This file will simply contain a main method
calling the code to be tested, which will be the main method for the Moving AI
maps. For this executable to created, we pass several options to the compiler:
to be compiled with level 2 optimizations (\texttt{-O2}) and using the
concurrent abstractions (\texttt{-threaded}), as well as to include all the
necessary runtime options (\texttt{-with-rtsopts}):

\begin{itemize}
\item \texttt{-N} sets the number of available processors to the number of
  processors of the system.
\item \texttt{-p} generates the profiling report.
\item \texttt{-s} includes garbage collection in the report.
\item \texttt{-h} includes memory usage in the report.
\item \texttt{-i} sets the profiler frequency to 0.1 seconds.  
\end{itemize}

Running \texttt{cabal build} will compile the project (and install all the
necessary profiling version of the dependencies) and generate the executable.
To run it, it is only necessary to run \texttt{cabal run profiling}. This
shortcut is much more robust and simple than to input the compiler flags every
time. Once run, it outputs a brief description of the profiling in the memory
and generates a comprehensive report (a \texttt{.prof} file) where the runtime
statistics are detailed. Now that the environment is set, we can proceed to
profile the code searching for the hot spots in the library.


\subsubsection{Finding the hot spots of the library}

\subsubsection{Profiling different \texttt{PriorityQueue} implementations}


After running the profiling experiments in the code, we have found that
problems with a fast growing queue generate a problematic space overhead, that
is translated to big garbage collection pauses that make execution slow. The
cost centers reveal that the space allocation happen most of all in the
\texttt{DataStructure} methods for adding nodes, so let us keep researching how
to fix this issue. To generate a lot of nodes in the open list, we will use a
bad heuristic in the problem \texttt{Warcraft}, to get a solution with these
statistics (obtained with the \texttt{Search.Monadic} library):

\begin{itemize}
 \item Number of expanded nodes: 378790
 \item Number of enqueued nodes: 734887
 \item Maximum length of the queue: 356099
\end{itemize}

Just to obtain a plan length of 19. This big size for the open list will
trigger the garbage collection problem and let us study it better. Running the
\texttt{profiling} executable of the library in this problem outputs, we can
see these results (please bear in mind the profiling overhead, stated as
\texttt{PROF} time):\\

\begin{lstlisting}
> cabal run profiling
   4,452,597,512 bytes allocated in the heap
   5,673,545,840 bytes copied during GC
     234,847,840 bytes maximum residency (45 sample(s))
       1,616,992 bytes maximum slop
             472 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      8573 colls,     0 par    1.030s   1.050s     0.0001s    0.0004s
  Gen  1        45 colls,     0 par    7.107s   7.359s     0.1635s    0.3832s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.001s  (  0.010s elapsed)
  MUT     time    3.333s  (  5.247s elapsed)
  GC      time    6.249s  (  6.495s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    1.888s  (  1.914s elapsed)
  EXIT    time    0.003s  (  0.046s elapsed)
  Total   time   11.475s  ( 11.797s elapsed)

  Alloc rate    1,335,764,960 bytes per MUT second

  Productivity  29.1% of total user, 28.6% of total elapsed
\end{lstlisting}


We can see how the profiler returns that only a 29.1\% of the time has been
spent in actual search computations, while more than half of the execution time
(54.5\%) has been spent in garbage collection. To check if this space leak is
caused by our implementation or by the library used to implement the priority
queue, we can experiment with different methods.\\

We can run the same problem using the implementation with the \texttt{psqueues}
(and more accurately the \texttt{Data.IntPSQ} module), an alternative library
that offers an implementation of a higher level than our original \texttt{heap}
package. The results are as follow:\\


\begin{lstlisting}
> cabal run profiling
   4,867,074,880 bytes allocated in the heap
   4,823,508,568 bytes copied during GC
     183,851,592 bytes maximum residency (46 sample(s))
       1,405,912 bytes maximum slop
             373 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      9345 colls,     0 par    0.992s   1.017s     0.0001s    0.0018s
  Gen  1        46 colls,     0 par    5.654s   5.849s     0.1271s    0.2936s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.001s  (  0.012s elapsed)
  MUT     time    3.508s  (  4.884s elapsed)
  GC      time    5.292s  (  5.498s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    1.354s  (  1.367s elapsed)
  EXIT    time    0.003s  (  0.037s elapsed)
  Total   time   10.159s  ( 10.431s elapsed)

  Alloc rate    1,387,545,376 bytes per MUT second

  Productivity  34.6% of total user, 34.1% of total elapsed
\end{lstlisting}


There, it can be seen that the garbage collection time has been reduced by a
second, but it is still a marginal improvement. Therefore, maybe a different
approach is needed: If we take a close look to the \texttt{addList} method, the
main cost center of this problem, we can see that we have two main options to
implement it: the first of them is to create a new \texttt{MinPrioHeap} and use
the library function \texttt{union} to merge both of them inside the new
structure. The code is shown in the listing \ref{addlist:union}.\\

\begin{lstlisting}[style=haskell, caption= \texttt{addList} using
  \texttt{union}, label=addlist:union]
addList xs (PriorityQueue h f) = PriorityQueue (union h xs') f
  where xs' = fromList $ map (\n -> (f n, n)) xs
\end{lstlisting}

Although the language should manage the memory correctly, maybe this is not the
best way of managing this operation. As a possible solution, we can try to
force the reutilization of the old structure by adding the nodes using a strict
fold over the \texttt{MinPrioHeap}. This implementation is shown in the listing
\ref{addlist:fold}.\\

\begin{lstlisting}[style=haskell, caption= \texttt{addList} using
  \texttt{fold'}, label=addlist:fold]
addList xs (PriorityQueue h f) = PriorityQueue (foldl' insertNode h xs) f
  where insertNode acc x = insert (f x, x) acc
\end{lstlisting}

To check the performance of all possibilities, the profiling problem stated at
the beginning of this subsection has been run using all combination of
implementations and library, and the full results of performance are found in
the chart shown in the figure \ref{chart-libraries}.\\

\begin{figure}[!htbp]
  \centering
\begin{tikzpicture}
  \begin{axis}[
    xbar,
    y axis line style = { opacity = 0 },
    axis x line       = none,
    % width = 8cm, height = 4cm,
    ytick = data,
    xmin=0,
    tickwidth         = 0pt,
    enlarge y limits  = 1,
    enlarge x limits  = 0.02,
    symbolic y coords = {Package A,Package B},
    nodes near coords,
  ]
  \addplot coordinates { (4.21,Package A) (4.52,Package B) };
  \addplot coordinates { (4.1,Package A) (4.8,Package B) };
  \legend{Using union, Using a fold}
  \end{axis}
\end{tikzpicture}
\vspace{-1cm}
\caption{Comparing Package A (\texttt{heap}) and Package B
  (\texttt{psqueues}).}
\label{chart-libraries}
\end{figure}

It can be seen how the performances are very similar to each other: no matter
which library or what implementation we use. We can then conclude that the
problem is intrinsic to the memory management performed by the RTS interface in
Haskell, or by how the compiler is turning the high-level declarations into
machine code.\\

Nonetheless, taking into account the knowledge that has been obtained by
profiling the data structure, the most intuitive and possible reason for this
space leak is that in Haskell, all data in immutable, and by adding the nodes
to the queue a complete new queue is being generated, allocating a new copy of
it and forcing the garbage collector to claim the old queue's space. Although
this is the natural behavior expected from the language, there are workarounds
to make a data structure mutable, but it has been impossible to find an
implementation that allows for a low complexity insertion and extraction of
nodes in a sorted heap and that is mutable. Implementing such data structure in
Haskell is not an easy task, and it is considered to be out of the scope of
this thesis.\\

\newpage

%%% Local Variables:
%%% TeX-master: "tfg"
%%% End: